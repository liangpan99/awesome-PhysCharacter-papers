# awesome-PhysCharacter-papers

*Last updated: Mar. 26, 2024*

<!-- - 

- MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations, ***arXiv 2023***. [[Website]](https://pku-mocca.github.io/MoConVQ-page/) [[Paper]](https://arxiv.org/abs/2310.10198) [[Code]](https://github.com/PKU-MoCCA/MoConVQ)

-->

## üî• News

- PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction, ***arXiv 2023***. [[Website]](https://wyhuai.github.io/physhoi-page/) [[Paper]](https://arxiv.org/abs/2312.04393) [[Code]](https://github.com/wyhuai/PhysHOI)

- MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations, ***arXiv 2023***. [[Website]](https://pku-mocca.github.io/MoConVQ-page/) [[Paper]](https://arxiv.org/abs/2310.10198) [[Code]](https://github.com/PKU-MoCCA/MoConVQ)

- Universal Humanoid Motion Representations for Physics-Based Control, ***arXiv 2023***. [[Website]](https://zhengyiluo.github.io/PULSE/) [[Paper]](https://arxiv.org/abs/2310.04582v1) [Code]


## üîç Papers

### 2023

- InsActor: Instruction-driven Physics-based Characters, ***NeurIPS 2023***. [[Website]](https://jiawei-ren.github.io/projects/insactor/index.html) [[Paper]](https://openreview.net/pdf?id=hXevuspQnX) [[Code]](https://github.com/jiawei-ren/insactor)

- AdaptNet: Policy Adaptation for Physics-Based Character Control, ***SIGGRAPH Asia 2023 (Journal Track)*** [[Website]](https://motion-lab.github.io/AdaptNet/) [[Paper]](https://arxiv.org/abs/2310.00239) [[Code]](https://github.com/xupei0610/AdaptNet)

- Neural Categorical Priors for Physics-Based Character Control, ***SIGGRAPH Asia 2023 (Journal Track)*** [[Website]](https://tencent-roboticsx.github.io/NCP/) [[Paper]](https://arxiv.org/abs/2308.07200) [Code]


## Vision-Language Models as Reward Models

- Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning, ***ICLR 2024*** [[Website]](https://sites.google.com/view/vlm-rm) [[Paper]](https://openreview.net/forum?id=N0I2RtD8je) [[Code]](https://github.com/AlignmentResearch/vlmrm)

- AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents, ***CVPR 2024*** [[Website]](https://anyskill.github.io/) [[Paper]](https://arxiv.org/abs/2403.12835) [[Code]](https://github.com/jiemingcui/anyskill)


## Benchmarks

- LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion, ***NeurIPS 2023 (Workshop)*** [[Website]](https://loco-mujoco.readthedocs.io/en/latest/) [[Paper]](https://arxiv.org/abs/2311.02496) [[Code]](https://github.com/robfiras/loco-mujoco)

- HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation, ***arXiv 2024*** [[Website]](https://humanoid-bench.github.io/) [[Paper]](https://arxiv.org/abs/2403.10506) [[Code]](https://github.com/carlosferrazza/humanoid-bench)